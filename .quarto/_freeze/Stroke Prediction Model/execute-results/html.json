{
  "hash": "dd76dacd867d5d1886bda2cf76d64bf4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Stroke Prediction Model\"\nformat: html\neditor: visual\n---\n\n## Introduction\n\nI will be building several models to predict heart disease. I will be using the Heart Failure Prediction Dataset from \\[Kaggle\\] (https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction).\n\nThis web page will be split into several chunks. The first will be for data description and preparation, and the next several will be for each model that I subsequently add.\n\nI will first use a random forest model, then will update with more models periodically.\n\n## Part One: Data Description, Processing, and Preparation\n\nPart one is common to all the models. It's quite brief and simply goes through the pre-processing and data cleaning.\n\n### STEP ONE: LOADING LIBRARIES AND DATA\n\nThis step is pretty self-explanatory. We will make use of six different R libraries for our initial analysis and model. First, `tidymodels` is a collection of packages for modeling and machine learning using `tidyverse` principles. The `workflows` library lets us use workflow container objects that are useful for modeling and analysis projects. The `tune` library will let us iterate through and find the best parameters for our models. The `ranger` library gives us access to a fast and efficient random forest algorithm. Lastly, `ggplot2` lets us make convenient and nice-looking plots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n✔ broom        1.0.9     ✔ recipes      1.3.1\n✔ dials        1.4.1     ✔ rsample      1.3.1\n✔ dplyr        1.1.4     ✔ tibble       3.3.0\n✔ ggplot2      3.5.2     ✔ tidyr        1.3.1\n✔ infer        1.0.9     ✔ tune         1.3.0\n✔ modeldata    1.5.1     ✔ workflows    1.3.0\n✔ parsnip      1.3.2     ✔ workflowsets 1.1.1\n✔ purrr        1.1.0     ✔ yardstick    1.3.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.4     ✔ stringr   1.5.1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(workflows)\nlibrary(tune)\nlibrary(ranger)\nlibrary(ggplot2)\n\norig_data <- RKaggle::get_dataset('fedesoriano/heart-failure-prediction')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 918 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Sex, ChestPainType, RestingECG, ExerciseAngina, ST_Slope\ndbl (7): Age, RestingBP, Cholesterol, FastingBS, MaxHR, Oldpeak, HeartDisease\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\n### STEP TWO: PRELIMINARY EDA, DATA IMPUTATION\n\nLet's take a glance at our data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(orig_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 918\nColumns: 12\n$ Age            <dbl> 40, 49, 37, 48, 54, 39, 45, 54, 37, 48, 37, 58, 39, 49,…\n$ Sex            <chr> \"M\", \"F\", \"M\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\", …\n$ ChestPainType  <chr> \"ATA\", \"NAP\", \"ATA\", \"ASY\", \"NAP\", \"NAP\", \"ATA\", \"ATA\",…\n$ RestingBP      <dbl> 140, 160, 130, 138, 150, 120, 130, 110, 140, 120, 130, …\n$ Cholesterol    <dbl> 289, 180, 283, 214, 195, 339, 237, 208, 207, 284, 211, …\n$ FastingBS      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ RestingECG     <chr> \"Normal\", \"Normal\", \"ST\", \"Normal\", \"Normal\", \"Normal\",…\n$ MaxHR          <dbl> 172, 156, 98, 108, 122, 170, 170, 142, 130, 120, 142, 9…\n$ ExerciseAngina <chr> \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", …\n$ Oldpeak        <dbl> 0.0, 1.0, 0.0, 1.5, 0.0, 0.0, 0.0, 0.0, 1.5, 0.0, 0.0, …\n$ ST_Slope       <chr> \"Up\", \"Flat\", \"Up\", \"Flat\", \"Up\", \"Up\", \"Up\", \"Up\", \"Fl…\n$ HeartDisease   <dbl> 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1…\n```\n\n\n:::\n:::\n\n\nSo, our data has 12 columns. I'll note that our output column (`HeartDisease`) is in 0s and 1s, representing negative and positive for heart disease, respectively. This is what we want to predict.\n\nWe have two different column types, double and character. Let's take a look at the character variables in the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\norig_data %>% select(where(is.character)) %>% apply(2, table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$Sex\n\n  F   M \n193 725 \n\n$ChestPainType\n\nASY ATA NAP  TA \n496 173 203  46 \n\n$RestingECG\n\n   LVH Normal     ST \n   188    552    178 \n\n$ExerciseAngina\n\n  N   Y \n547 371 \n\n$ST_Slope\n\nDown Flat   Up \n  63  460  395 \n```\n\n\n:::\n:::\n\n\nLet's go through these variables. The labeling of each variable is available on the Kaggle Data Card.\n\n`Sex` is skewed towards males; this might be something worth considering when inferring from this dataset.\n\nThere are four different chest pain types (`ChestPainType`) - asymptomatic (ASY), atypical angina (ATA), non-anginal pain (NAP), and typical angina (TA).\n\n`RestingECG` is either LVH (showing probable or definite left ventricular hypertrophy by Estes' criteria), Normal, or ST (having ST-T wave abnormality - T wave inversions and/or ST elevation or depression of \\> 0.05 mV).\n\n`ExerciseAngina` indicates whether or not the patient has exercise-induced angina.\n\nFinally, `ST_Slope` indicates the slope of the peak exercise ST segment (Up: upsloping, Flat: flat, Down: downsloping).\n\nLet's make a new, cleaned data frame where we convert all character features and the output feature to factors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinalData <- orig_data %>% mutate(HeartDisease=as.factor(HeartDisease)) %>% \n\tmutate(across(where(is.character), as.factor))\n```\n:::\n\n\nNow, let's take a look at the non-factor variables.\n\n(Note: figuring out how to loop through and graph numeric variables was a huge pain! Shout-out to ggplot for making visualization easier).\n\nTo make these graphs, we first select the relevant variables, then work the data frame into two columns (this will help us separate by each variable). Then, we can make a plot with ggplot to visualize the distribution of our numeric variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\norig_data %>% select(where(is.double)) %>% pivot_longer(cols=everything()) %>% \n\tggplot(aes(x=value, fill=name)) + \n\tfacet_wrap(~ name, scales = 'free') + geom_histogram(bins=30)\n```\n\n::: {.cell-output-display}\n![](Stroke-Prediction-Model_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nIt seems like `Age`, `maxHR`, and `RestingBP` are approximately normally distributed. `RestingBP` seemingly has an outlier at 0 (a dead person was counted in the data, probably). `Cholesterol` has a spike at 0, corresponding to missing data. We can probably leave `Oldpeak` as it is. Lastly, `FastingBS` is another binary data type, so we should convert it to factor so it is better represented in our data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinalData <- finalData %>% mutate(FastingBS=as.factor(FastingBS))\n\nfinalData <- finalData %>% mutate(across(Cholesterol, function(i) {\n\t\tif_else(i==0, as.numeric(NA), i)\n\t}))\n```\n:::\n\n\n## Part Two: Random Forest Model\n\n### STEP ONE: CREATING THE MODEL FRAMEWORK\n\nOkay, now we have to split our original dataset into the training and testing subparts. Tidymodels provides a simple framework to do this with `initial_split()`. We also make a cross validation object which will come in handy later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123456)\ndataSplit <- initial_split(finalData, prop = 3/4)\ndataTraining <- training(dataSplit)\ndataTesting <- testing(dataSplit)\ndataCV <- vfold_cv(dataTraining)\ndataCV\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits           id    \n   <list>           <chr> \n 1 <split [619/69]> Fold01\n 2 <split [619/69]> Fold02\n 3 <split [619/69]> Fold03\n 4 <split [619/69]> Fold04\n 5 <split [619/69]> Fold05\n 6 <split [619/69]> Fold06\n 7 <split [619/69]> Fold07\n 8 <split [619/69]> Fold08\n 9 <split [620/68]> Fold09\n10 <split [620/68]> Fold10\n```\n\n\n:::\n:::\n\n\nOkay, great. So we have two new datasets selected at random (`dataTraining` and `dataTesting`) and a cross-validation object. Now, we have to create the framework of the model. We'll do this with the `recipe()` function from tidymodels. Crucially, we specify the model to normalize all numeric columns (subtract the mean of each column and divide by its standard deviation) and impute the missing values in the `Cholesterol` column using the k-nearest neighbors method.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataRecipe <- recipe(HeartDisease ~ ., data=finalData) %>% \n\tstep_normalize(all_numeric()) %>% step_impute_knn(Cholesterol)\n```\n:::\n\n\nSo now we have our \"recipe\" to make the model; it's time to follow through. The `rand_forest()` function initializes a random forest model, then we set parameters with `set_args()`. We set `mtry` (the number of variables evaluated per tree) and `min_n` (the minimum number of observations needed in a node to split the node further - the lower the value, the deeper the tree) to `tune()` - this will allow us to iterate through these values on our cross validation datasets. The `importance` parameter says that we will define the importance of each variable by permutation (I will explain what this is later). Lastly, we tell the model that we want classification (0 or 1) instead of regression (continuous).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrfModel <- rand_forest() %>% set_args(mtry=tune(), min_n=tune()) %>% \n\tset_engine('ranger', importance = 'permutation') %>% \n\tset_mode('classification')\n```\n:::\n\n\nWe make a `workflow` object with our recipe and our model. Let's take a look at it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrfWorkflow <- workflow() %>% add_recipe(dataRecipe) %>% add_model(rfModel)\nrfWorkflow\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_normalize()\n• step_impute_knn()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  min_n = tune()\n\nEngine-Specific Arguments:\n  importance = permutation\n\nComputational engine: ranger \n```\n\n\n:::\n:::\n\n\n### STEP TWO: TESTING DIFFERENT PARAMETERS\n\nThe reason I tune()-d the `mtry` and `min_n` parameters is because these might significantly influence the accuracy and dependability of our model. Now that we have all the required frameworks, we can test parameter values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Good mtry is sqrt(total vars) = sqrt(11) = 3\nrfGrid <- expand.grid(mtry=2:6, min_n=c(1, 3, 5, 10, 15, 20, 30))\nrfTuneResults <- rfWorkflow %>% tune_grid(resamples=dataCV, grid=rfGrid, \n\tmetrics=metric_set(accuracy, roc_auc))\nrfTuneResults %>% collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 70 × 8\n    mtry min_n .metric  .estimator  mean     n std_err .config              \n   <int> <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1     2     1 accuracy binary     0.871    10 0.0113  Preprocessor1_Model01\n 2     2     1 roc_auc  binary     0.924    10 0.00980 Preprocessor1_Model01\n 3     3     1 accuracy binary     0.865    10 0.0120  Preprocessor1_Model02\n 4     3     1 roc_auc  binary     0.922    10 0.0109  Preprocessor1_Model02\n 5     4     1 accuracy binary     0.866    10 0.0134  Preprocessor1_Model03\n 6     4     1 roc_auc  binary     0.920    10 0.0107  Preprocessor1_Model03\n 7     5     1 accuracy binary     0.866    10 0.0144  Preprocessor1_Model04\n 8     5     1 roc_auc  binary     0.917    10 0.0111  Preprocessor1_Model04\n 9     6     1 accuracy binary     0.858    10 0.0155  Preprocessor1_Model05\n10     6     1 roc_auc  binary     0.914    10 0.0112  Preprocessor1_Model05\n# ℹ 60 more rows\n```\n\n\n:::\n:::\n\n\nI'm interested in the effect of `mtry`. Let's take a look across various `min_n`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrfTuneResults %>% collect_metrics() %>% filter(min_n %in% c(1,3,30), .metric=='accuracy')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 15 × 8\n    mtry min_n .metric  .estimator  mean     n std_err .config              \n   <int> <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1     2     1 accuracy binary     0.871    10  0.0113 Preprocessor1_Model01\n 2     3     1 accuracy binary     0.865    10  0.0120 Preprocessor1_Model02\n 3     4     1 accuracy binary     0.866    10  0.0134 Preprocessor1_Model03\n 4     5     1 accuracy binary     0.866    10  0.0144 Preprocessor1_Model04\n 5     6     1 accuracy binary     0.858    10  0.0155 Preprocessor1_Model05\n 6     2     3 accuracy binary     0.874    10  0.0133 Preprocessor1_Model06\n 7     3     3 accuracy binary     0.872    10  0.0121 Preprocessor1_Model07\n 8     4     3 accuracy binary     0.869    10  0.0149 Preprocessor1_Model08\n 9     5     3 accuracy binary     0.865    10  0.0143 Preprocessor1_Model09\n10     6     3 accuracy binary     0.865    10  0.0128 Preprocessor1_Model10\n11     2    30 accuracy binary     0.861    10  0.0150 Preprocessor1_Model31\n12     3    30 accuracy binary     0.862    10  0.0140 Preprocessor1_Model32\n13     4    30 accuracy binary     0.865    10  0.0157 Preprocessor1_Model33\n14     5    30 accuracy binary     0.866    10  0.0163 Preprocessor1_Model34\n15     6    30 accuracy binary     0.868    10  0.0157 Preprocessor1_Model35\n```\n\n\n:::\n:::\n\n\nComparing `mtry`'s for each `min_n`, it seems like there is a small, weak negative correlation between `mtry` and model accuracy. However, `min_n` seems a lot more interesting. Let's keep experimenting. So, let's select the smallest `mtry` (2 and let's try 1 as well) and try a larger range of `min_n`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrfGrid2 <- expand.grid(mtry=1:2, min_n=c(1, 3, 5, seq(5, 50, 5)))\nrfTuneResults2 <- rfWorkflow %>% tune_grid(resamples=dataCV, grid=rfGrid2, \n\tmetrics=metric_set(accuracy, roc_auc))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Duplicate rows in grid of tuning combinations found and removed.\n```\n\n\n:::\n\n```{.r .cell-code}\nrfTuneResults2 %>% collect_metrics() %>% filter(.metric=='accuracy') %>% \n\tarrange(desc(mean))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 24 × 8\n    mtry min_n .metric  .estimator  mean     n std_err .config              \n   <int> <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1     2     5 accuracy binary     0.874    10  0.0118 Preprocessor1_Model06\n 2     2    15 accuracy binary     0.874    10  0.0141 Preprocessor1_Model10\n 3     2    10 accuracy binary     0.871    10  0.0132 Preprocessor1_Model08\n 4     2    20 accuracy binary     0.871    10  0.0150 Preprocessor1_Model12\n 5     2     3 accuracy binary     0.871    10  0.0129 Preprocessor1_Model04\n 6     1     3 accuracy binary     0.869    10  0.0147 Preprocessor1_Model03\n 7     2     1 accuracy binary     0.868    10  0.0142 Preprocessor1_Model02\n 8     2    25 accuracy binary     0.868    10  0.0152 Preprocessor1_Model14\n 9     1     5 accuracy binary     0.866    10  0.0139 Preprocessor1_Model05\n10     1    10 accuracy binary     0.866    10  0.0135 Preprocessor1_Model07\n# ℹ 14 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nrfTuneResults2 %>% collect_metrics() %>% filter(.metric=='roc_auc') %>% \n\tarrange(desc(mean))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 24 × 8\n    mtry min_n .metric .estimator  mean     n std_err .config              \n   <int> <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n 1     2    50 roc_auc binary     0.929    10 0.00971 Preprocessor1_Model24\n 2     2    40 roc_auc binary     0.928    10 0.0100  Preprocessor1_Model20\n 3     2    20 roc_auc binary     0.927    10 0.00985 Preprocessor1_Model12\n 4     2    45 roc_auc binary     0.927    10 0.0103  Preprocessor1_Model22\n 5     2    35 roc_auc binary     0.927    10 0.0103  Preprocessor1_Model18\n 6     2    30 roc_auc binary     0.927    10 0.0105  Preprocessor1_Model16\n 7     1    10 roc_auc binary     0.927    10 0.0104  Preprocessor1_Model07\n 8     1     5 roc_auc binary     0.926    10 0.0102  Preprocessor1_Model05\n 9     1     3 roc_auc binary     0.926    10 0.0108  Preprocessor1_Model03\n10     2    25 roc_auc binary     0.926    10 0.0103  Preprocessor1_Model14\n# ℹ 14 more rows\n```\n\n\n:::\n:::\n\n\nmtry=2, min_n=5 comes in at first place for accuracy, first (tied) for roc_auc, and has lowest std error for accuracy and one of the lowest for roc_auc. It's safe to say that this model is our winner. Turns out we didn't need to investigate again after all!\n\nNow, let's select our winner model and apply it to our workflow.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparamFinal <- rfTuneResults2 %>% select_best(metric='accuracy')\nrfWorkflow <- rfWorkflow %>% finalize_workflow(paramFinal)\n```\n:::\n\n\n### STEP THREE: FIT THE MODEL\n\nSo now we have our workflow:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrfWorkflow\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_normalize()\n• step_impute_knn()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = 2\n  min_n = 5\n\nEngine-Specific Arguments:\n  importance = permutation\n\nComputational engine: ranger \n```\n\n\n:::\n:::\n\n\nNow we can fit this model onto our split data. Tidymodels is nice because it lets you automatically train on the training data and then evaluate on the testing data using the `initial_split()` object. Without further ado, let's fit it and see how it performs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrfFit <- rfWorkflow %>% last_fit(dataSplit)\ntestPerformance <- rfFit %>% collect_metrics()\ntestPerformance\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  <chr>       <chr>          <dbl> <chr>               \n1 accuracy    binary         0.861 Preprocessor1_Model1\n2 roc_auc     binary         0.920 Preprocessor1_Model1\n3 brier_class binary         0.108 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\nAccuracy of 86.1%, not bad!\n\nLet's visualize our results. We'll make a confusion matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrfFit %>% collect_predictions() %>% conf_mat(truth=HeartDisease, \n\testimate=.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction   0   1\n         0  81  14\n         1  18 117\n```\n\n\n:::\n:::\n\n\nOur model on the testing data resulted in 81 true negatives, 117 true positive, 14 false negatives, and 18 false positives.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrfFit %>% collect_predictions() %>% ggplot() + \n\tgeom_density(aes(x=.pred_1, fill=HeartDisease, alpha=0.5))\n```\n\n::: {.cell-output-display}\n![](Stroke-Prediction-Model_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nWe can see here that we have some pretty neat-looking separation. The plot colors positive outcomes in blue and negative in red. We can see that the model is pretty good at separating the positives from the negatives.\n\n### STEP FOUR: FIT ON A TARGET DATASET\n\nNow we can fit our final model on the dataset and analyze the importance of each variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinalModel <- fit(rfWorkflow, finalData)\nextract_fit_parsnip(finalModel)$fit$variable.importance\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Age            Sex  ChestPainType      RestingBP    Cholesterol \n  0.0079340368   0.0158278641   0.0334637354   0.0032045429   0.0009649963 \n     FastingBS     RestingECG          MaxHR ExerciseAngina        Oldpeak \n  0.0109401291   0.0020280590   0.0171043748   0.0235892797   0.0282305146 \n      ST_Slope \n  0.0914582680 \n```\n\n\n:::\n:::\n\n\nRemember our \"permutation\" argument from before? To find variable `importance`, the model takes each variable one by one, shuffles its values (effectively removing its correlation to the outcome), and evaluates the decrease in the model metric (in this case, accuracy).\n\nTo conclude, our model performed quite well with 86.1% accuracy, an area of 0.920 under the ROC curve, and a Brier score of 0.108. `ST_Slope` seems to be the most important variable, accounting for \\~9% of the model's predictive power.\n",
    "supporting": [
      "Stroke-Prediction-Model_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}