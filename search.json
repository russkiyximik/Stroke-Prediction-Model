[
  {
    "objectID": "Stroke Prediction Model.html",
    "href": "Stroke Prediction Model.html",
    "title": "Stroke Prediction Model",
    "section": "",
    "text": "I will be building several models to predict heart disease. I will be using the Heart Failure Prediction Dataset from [Kaggle] (https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction).\nThis web page will be split into several chunks. The first will be for data description and preparation, and the next several will be for each model that I subsequently add.\nI will first use a random forest model, then will update with more models periodically."
  },
  {
    "objectID": "Stroke Prediction Model.html#introduction",
    "href": "Stroke Prediction Model.html#introduction",
    "title": "Stroke Prediction Model",
    "section": "",
    "text": "I will be building several models to predict heart disease. I will be using the Heart Failure Prediction Dataset from [Kaggle] (https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction).\nThis web page will be split into several chunks. The first will be for data description and preparation, and the next several will be for each model that I subsequently add.\nI will first use a random forest model, then will update with more models periodically."
  },
  {
    "objectID": "Stroke Prediction Model.html#part-one-data-description-processing-and-preparation",
    "href": "Stroke Prediction Model.html#part-one-data-description-processing-and-preparation",
    "title": "Stroke Prediction Model",
    "section": "Part One: Data Description, Processing, and Preparation",
    "text": "Part One: Data Description, Processing, and Preparation\nPart one is common to all the models. It’s quite brief and simply goes through the pre-processing and data cleaning.\n\nSTEP ONE: LOADING LIBRARIES AND DATA\nThis step is pretty self-explanatory. We will make use of six different R libraries for our initial analysis and model. First, tidymodels is a collection of packages for modeling and machine learning using tidyverse principles. The workflows library lets us use workflow container objects that are useful for modeling and analysis projects. The tune library will let us iterate through and find the best parameters for our models. The ranger library gives us access to a fast and efficient random forest algorithm. Lastly, ggplot2 lets us make convenient and nice-looking plots.\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(workflows)\nlibrary(tune)\nlibrary(ranger)\nlibrary(ggplot2)\n\norig_data &lt;- RKaggle::get_dataset('fedesoriano/heart-failure-prediction')\n\n\n\nSTEP TWO: PRELIMINARY EDA, DATA IMPUTATION\nLet’s take a glance at our data.\n\nglimpse(orig_data)\n\nRows: 918\nColumns: 12\n$ Age            &lt;dbl&gt; 40, 49, 37, 48, 54, 39, 45, 54, 37, 48, 37, 58, 39, 49,…\n$ Sex            &lt;chr&gt; \"M\", \"F\", \"M\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\", …\n$ ChestPainType  &lt;chr&gt; \"ATA\", \"NAP\", \"ATA\", \"ASY\", \"NAP\", \"NAP\", \"ATA\", \"ATA\",…\n$ RestingBP      &lt;dbl&gt; 140, 160, 130, 138, 150, 120, 130, 110, 140, 120, 130, …\n$ Cholesterol    &lt;dbl&gt; 289, 180, 283, 214, 195, 339, 237, 208, 207, 284, 211, …\n$ FastingBS      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ RestingECG     &lt;chr&gt; \"Normal\", \"Normal\", \"ST\", \"Normal\", \"Normal\", \"Normal\",…\n$ MaxHR          &lt;dbl&gt; 172, 156, 98, 108, 122, 170, 170, 142, 130, 120, 142, 9…\n$ ExerciseAngina &lt;chr&gt; \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", …\n$ Oldpeak        &lt;dbl&gt; 0.0, 1.0, 0.0, 1.5, 0.0, 0.0, 0.0, 0.0, 1.5, 0.0, 0.0, …\n$ ST_Slope       &lt;chr&gt; \"Up\", \"Flat\", \"Up\", \"Flat\", \"Up\", \"Up\", \"Up\", \"Up\", \"Fl…\n$ HeartDisease   &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1…\n\n\nSo, our data has 12 columns. I’ll note that our output column (HeartDisease) is in 0s and 1s, representing negative and positive for heart disease, respectively. This is what we want to predict.\nWe have two different column types, double and character. Let’s take a look at the character variables in the dataset.\n\norig_data %&gt;% select(where(is.character)) %&gt;% apply(2, table)\n\n$Sex\n\n  F   M \n193 725 \n\n$ChestPainType\n\nASY ATA NAP  TA \n496 173 203  46 \n\n$RestingECG\n\n   LVH Normal     ST \n   188    552    178 \n\n$ExerciseAngina\n\n  N   Y \n547 371 \n\n$ST_Slope\n\nDown Flat   Up \n  63  460  395 \n\n\nLet’s go through these variables. The labeling of each variable is available on the Kaggle Data Card.\nSex is skewed towards males; this might be something worth considering when inferring from this dataset.\nThere are four different chest pain types (ChestPainType) - asymptomatic (ASY), atypical angina (ATA), non-anginal pain (NAP), and typical angina (TA).\nRestingECG is either LVH (showing probable or definite left ventricular hypertrophy by Estes’ criteria), Normal, or ST (having ST-T wave abnormality - T wave inversions and/or ST elevation or depression of &gt; 0.05 mV).\nExerciseAngina indicates whether or not the patient has exercise-induced angina.\nFinally, ST_Slope indicates the slope of the peak exercise ST segment (Up: upsloping, Flat: flat, Down: downsloping).\nLet’s make a new, cleaned data frame where we convert all character features and the output feature to factors.\n\nfinalData &lt;- orig_data %&gt;% mutate(HeartDisease=as.factor(HeartDisease)) %&gt;% \n    mutate(across(where(is.character), as.factor))\n\nNow, let’s take a look at the non-factor variables.\n(Note: figuring out how to loop through and graph numeric variables was a huge pain! Shout-out to ggplot for making visualization easier).\nTo make these graphs, we first select the relevant variables, then work the data frame into two columns (this will help us separate by each variable). Then, we can make a plot with ggplot to visualize the distribution of our numeric variables.\n\norig_data %&gt;% select(where(is.double)) %&gt;% pivot_longer(cols=everything()) %&gt;% \n    ggplot(aes(x=value, fill=name)) + \n    facet_wrap(~ name, scales = 'free') + geom_histogram(bins=30)\n\n\n\n\n\n\n\n\nIt seems like Age, maxHR, and RestingBP are approximately normally distributed. RestingBP seemingly has an outlier at 0 (a dead person was counted in the data, probably). Cholesterol has a spike at 0, corresponding to missing data. We can probably leave Oldpeak as it is. Lastly, FastingBS is another binary data type, so we should convert it to factor so it is better represented in our data.\n\nfinalData &lt;- finalData %&gt;% mutate(FastingBS=as.factor(FastingBS))\n\nfinalData &lt;- finalData %&gt;% mutate(across(Cholesterol, function(i) {\n        if_else(i==0, as.numeric(NA), i)\n    }))\n\nLastly, let’s check if any our variables are correlated; if they are, they can increase redundancy and make the model more complex, therefore increasing the risk of overfitting.\n\nfinalData %&gt;% select(where(is.numeric)) %&gt;% cor\n\n                   Age  RestingBP Cholesterol      MaxHR    Oldpeak\nAge          1.0000000  0.2543994          NA -0.3820447  0.2586115\nRestingBP    0.2543994  1.0000000          NA -0.1121350  0.1648030\nCholesterol         NA         NA           1         NA         NA\nMaxHR       -0.3820447 -0.1121350          NA  1.0000000 -0.1606906\nOldpeak      0.2586115  0.1648030          NA -0.1606906  1.0000000\n\n\nThankfully, none of our features are highly correlated. The highest (absolute) correlation is between Age and Maximum Heart Rate, which makes sense. But we can ignore it."
  },
  {
    "objectID": "Stroke Prediction Model.html#part-two-random-forest-model",
    "href": "Stroke Prediction Model.html#part-two-random-forest-model",
    "title": "Stroke Prediction Model",
    "section": "Part Two: Random Forest Model",
    "text": "Part Two: Random Forest Model\n\nSTEP ONE: CREATING THE MODEL FRAMEWORK\nOkay, now we have to split our original dataset into the training and testing subparts. Tidymodels provides a simple framework to do this with initial_split(). We also make a cross validation object which will come in handy later.\n\nset.seed(123456)\ndataSplit &lt;- initial_split(finalData, prop = 3/4)\ndataTraining &lt;- training(dataSplit)\ndataTesting &lt;- testing(dataSplit)\ndataCV &lt;- vfold_cv(dataTraining)\ndataCV\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits           id    \n   &lt;list&gt;           &lt;chr&gt; \n 1 &lt;split [619/69]&gt; Fold01\n 2 &lt;split [619/69]&gt; Fold02\n 3 &lt;split [619/69]&gt; Fold03\n 4 &lt;split [619/69]&gt; Fold04\n 5 &lt;split [619/69]&gt; Fold05\n 6 &lt;split [619/69]&gt; Fold06\n 7 &lt;split [619/69]&gt; Fold07\n 8 &lt;split [619/69]&gt; Fold08\n 9 &lt;split [620/68]&gt; Fold09\n10 &lt;split [620/68]&gt; Fold10\n\n\nOkay, great. So we have two new datasets selected at random (dataTraining and dataTesting) and a cross-validation object. Now, we have to create the framework of the model. We’ll do this with the recipe() function from tidymodels. Crucially, we specify the model to normalize all numeric columns (subtract the mean of each column and divide by its standard deviation) and impute the missing values in the Cholesterol column using the k-nearest neighbors method.\n\ndataRecipe &lt;- recipe(HeartDisease ~ ., data=finalData) %&gt;% \n    step_normalize(all_numeric()) %&gt;% step_impute_knn(Cholesterol)\n\nSo now we have our “recipe” to make the model; it’s time to follow through. The rand_forest() function initializes a random forest model, then we set parameters with set_args(). We set mtry (the number of variables evaluated per tree) and min_n (the minimum number of observations needed in a node to split the node further - the lower the value, the deeper the tree) to tune() - this will allow us to iterate through these values on our cross validation datasets. The importance parameter says that we will define the importance of each variable by permutation (I will explain what this is later). Lastly, we tell the model that we want classification (0 or 1) instead of regression (continuous).\n\nrfModel &lt;- rand_forest() %&gt;% set_args(mtry=tune(), min_n=tune()) %&gt;% \n    set_engine('ranger', importance = 'permutation') %&gt;% \n    set_mode('classification')\n\nWe make a workflow object with our recipe and our model. Let’s take a look at it:\n\nrfWorkflow &lt;- workflow() %&gt;% add_recipe(dataRecipe) %&gt;% add_model(rfModel)\nrfWorkflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_normalize()\n• step_impute_knn()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  min_n = tune()\n\nEngine-Specific Arguments:\n  importance = permutation\n\nComputational engine: ranger \n\n\n\n\nSTEP TWO: TESTING DIFFERENT PARAMETERS\nThe reason I tune()-d the mtry and min_n parameters is because these might significantly influence the accuracy and dependability of our model. Now that we have all the required frameworks, we can test parameter values.\n\n# Good mtry is sqrt(total vars) = sqrt(11) = 3\nrfGrid &lt;- expand.grid(mtry=2:6, min_n=c(1, 3, 5, 10, 15, 20, 30))\nrfTuneResults &lt;- rfWorkflow %&gt;% tune_grid(resamples=dataCV, grid=rfGrid, \n    metrics=metric_set(accuracy, roc_auc))\nrfTuneResults %&gt;% collect_metrics()\n\n# A tibble: 70 × 8\n    mtry min_n .metric  .estimator  mean     n std_err .config              \n   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1     2     1 accuracy binary     0.871    10 0.0113  Preprocessor1_Model01\n 2     2     1 roc_auc  binary     0.924    10 0.00980 Preprocessor1_Model01\n 3     3     1 accuracy binary     0.865    10 0.0120  Preprocessor1_Model02\n 4     3     1 roc_auc  binary     0.922    10 0.0109  Preprocessor1_Model02\n 5     4     1 accuracy binary     0.866    10 0.0134  Preprocessor1_Model03\n 6     4     1 roc_auc  binary     0.920    10 0.0107  Preprocessor1_Model03\n 7     5     1 accuracy binary     0.866    10 0.0144  Preprocessor1_Model04\n 8     5     1 roc_auc  binary     0.917    10 0.0111  Preprocessor1_Model04\n 9     6     1 accuracy binary     0.858    10 0.0155  Preprocessor1_Model05\n10     6     1 roc_auc  binary     0.914    10 0.0112  Preprocessor1_Model05\n# ℹ 60 more rows\n\n\nI’m interested in the effect of mtry. Let’s take a look across various min_n:\n\nrfTuneResults %&gt;% collect_metrics() %&gt;% filter(min_n %in% c(1,3,30), .metric=='accuracy')\n\n# A tibble: 15 × 8\n    mtry min_n .metric  .estimator  mean     n std_err .config              \n   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1     2     1 accuracy binary     0.871    10  0.0113 Preprocessor1_Model01\n 2     3     1 accuracy binary     0.865    10  0.0120 Preprocessor1_Model02\n 3     4     1 accuracy binary     0.866    10  0.0134 Preprocessor1_Model03\n 4     5     1 accuracy binary     0.866    10  0.0144 Preprocessor1_Model04\n 5     6     1 accuracy binary     0.858    10  0.0155 Preprocessor1_Model05\n 6     2     3 accuracy binary     0.874    10  0.0133 Preprocessor1_Model06\n 7     3     3 accuracy binary     0.872    10  0.0121 Preprocessor1_Model07\n 8     4     3 accuracy binary     0.869    10  0.0149 Preprocessor1_Model08\n 9     5     3 accuracy binary     0.865    10  0.0143 Preprocessor1_Model09\n10     6     3 accuracy binary     0.865    10  0.0128 Preprocessor1_Model10\n11     2    30 accuracy binary     0.861    10  0.0150 Preprocessor1_Model31\n12     3    30 accuracy binary     0.862    10  0.0140 Preprocessor1_Model32\n13     4    30 accuracy binary     0.865    10  0.0157 Preprocessor1_Model33\n14     5    30 accuracy binary     0.866    10  0.0163 Preprocessor1_Model34\n15     6    30 accuracy binary     0.868    10  0.0157 Preprocessor1_Model35\n\n\nComparing mtry’s for each min_n, it seems like there is a small, weak negative correlation between mtry and model accuracy. However, min_n seems a lot more interesting. Let’s keep experimenting. So, let’s select the smallest mtry (2 and let’s try 1 as well) and try a larger range of min_n.\n\nrfGrid2 &lt;- expand.grid(mtry=1:2, min_n=c(1, 3, 5, seq(5, 50, 5)))\nrfTuneResults2 &lt;- rfWorkflow %&gt;% tune_grid(resamples=dataCV, grid=rfGrid2, \n    metrics=metric_set(accuracy, roc_auc))\n\nWarning: Duplicate rows in grid of tuning combinations found and removed.\n\nrfTuneResults2 %&gt;% collect_metrics() %&gt;% filter(.metric=='accuracy') %&gt;% \n    arrange(desc(mean))\n\n# A tibble: 24 × 8\n    mtry min_n .metric  .estimator  mean     n std_err .config              \n   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1     2     5 accuracy binary     0.874    10  0.0118 Preprocessor1_Model06\n 2     2    15 accuracy binary     0.874    10  0.0141 Preprocessor1_Model10\n 3     2    10 accuracy binary     0.871    10  0.0132 Preprocessor1_Model08\n 4     2    20 accuracy binary     0.871    10  0.0150 Preprocessor1_Model12\n 5     2     3 accuracy binary     0.871    10  0.0129 Preprocessor1_Model04\n 6     1     3 accuracy binary     0.869    10  0.0147 Preprocessor1_Model03\n 7     2     1 accuracy binary     0.868    10  0.0142 Preprocessor1_Model02\n 8     2    25 accuracy binary     0.868    10  0.0152 Preprocessor1_Model14\n 9     1     5 accuracy binary     0.866    10  0.0139 Preprocessor1_Model05\n10     1    10 accuracy binary     0.866    10  0.0135 Preprocessor1_Model07\n# ℹ 14 more rows\n\nrfTuneResults2 %&gt;% collect_metrics() %&gt;% filter(.metric=='roc_auc') %&gt;% \n    arrange(desc(mean))\n\n# A tibble: 24 × 8\n    mtry min_n .metric .estimator  mean     n std_err .config              \n   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1     2    50 roc_auc binary     0.929    10 0.00971 Preprocessor1_Model24\n 2     2    40 roc_auc binary     0.928    10 0.0100  Preprocessor1_Model20\n 3     2    20 roc_auc binary     0.927    10 0.00985 Preprocessor1_Model12\n 4     2    45 roc_auc binary     0.927    10 0.0103  Preprocessor1_Model22\n 5     2    35 roc_auc binary     0.927    10 0.0103  Preprocessor1_Model18\n 6     2    30 roc_auc binary     0.927    10 0.0105  Preprocessor1_Model16\n 7     1    10 roc_auc binary     0.927    10 0.0104  Preprocessor1_Model07\n 8     1     5 roc_auc binary     0.926    10 0.0102  Preprocessor1_Model05\n 9     1     3 roc_auc binary     0.926    10 0.0108  Preprocessor1_Model03\n10     2    25 roc_auc binary     0.926    10 0.0103  Preprocessor1_Model14\n# ℹ 14 more rows\n\n\nmtry=2, min_n=5 comes in at first place for accuracy, first (tied) for roc_auc, and has lowest std error for accuracy and one of the lowest for roc_auc. It’s safe to say that this model is our winner. Turns out we didn’t need to investigate again after all!\nNow, let’s select our winner model and apply it to our workflow.\n\nparamFinal &lt;- rfTuneResults2 %&gt;% select_best(metric='accuracy')\nrfWorkflow &lt;- rfWorkflow %&gt;% finalize_workflow(paramFinal)\n\n\n\nSTEP THREE: FIT THE MODEL\nSo now we have our workflow:\n\nrfWorkflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_normalize()\n• step_impute_knn()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = 2\n  min_n = 5\n\nEngine-Specific Arguments:\n  importance = permutation\n\nComputational engine: ranger \n\n\nNow we can fit this model onto our split data. Tidymodels is nice because it lets you automatically train on the training data and then evaluate on the testing data using the initial_split() object. Without further ado, let’s fit it and see how it performs:\n\nrfFit &lt;- rfWorkflow %&gt;% last_fit(dataSplit)\ntestPerformance &lt;- rfFit %&gt;% collect_metrics()\ntestPerformance\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.861 Preprocessor1_Model1\n2 roc_auc     binary         0.920 Preprocessor1_Model1\n3 brier_class binary         0.108 Preprocessor1_Model1\n\n\nAccuracy of 86.1%, not bad!\nLet’s visualize our results. We’ll make a confusion matrix:\n\nrfFit %&gt;% collect_predictions() %&gt;% conf_mat(truth=HeartDisease, \n    estimate=.pred_class)\n\n          Truth\nPrediction   0   1\n         0  81  14\n         1  18 117\n\n\nOur model on the testing data resulted in 81 true negatives, 117 true positive, 14 false negatives, and 18 false positives.\n\nrfFit %&gt;% collect_predictions() %&gt;% ggplot() + \n    geom_density(aes(x=.pred_1, fill=HeartDisease, alpha=0.5))\n\n\n\n\n\n\n\n\nWe can see here that we have some pretty neat-looking separation. The plot colors positive outcomes in blue and negative in red. We can see that the model is pretty good at separating the positives from the negatives.\n\n\nSTEP FOUR: FIT ON A TARGET DATASET\nNow we can fit our final model on the dataset and analyze the importance of each variable.\n\nfinalModel &lt;- fit(rfWorkflow, finalData)\nextract_fit_parsnip(finalModel)$fit$variable.importance\n\n           Age            Sex  ChestPainType      RestingBP    Cholesterol \n  0.0079340368   0.0158278641   0.0334637354   0.0032045429   0.0009649963 \n     FastingBS     RestingECG          MaxHR ExerciseAngina        Oldpeak \n  0.0109401291   0.0020280590   0.0171043748   0.0235892797   0.0282305146 \n      ST_Slope \n  0.0914582680 \n\n\nRemember our “permutation” argument from before? To find variable importance, the model takes each variable one by one, shuffles its values (effectively removing its correlation to the outcome), and evaluates the decrease in the model metric (in this case, accuracy).\nTo conclude, our model performed quite well with 86.1% accuracy, an area of 0.920 under the ROC curve, and a Brier score of 0.108. ST_Slope seems to be the most important variable, accounting for ~9% of the model’s predictive power."
  },
  {
    "objectID": "Stroke Prediction Model.html#part-three-boosted-trees-model",
    "href": "Stroke Prediction Model.html#part-three-boosted-trees-model",
    "title": "Stroke Prediction Model",
    "section": "Part Three: Boosted Trees Model",
    "text": "Part Three: Boosted Trees Model\n\nSTEP ONE: CREATING THE MODEL FRAMEWORK\nNow let’s make a boosted tree model for our data. We will be using the C5.0 algorithm, made convenient through R. Briefly, it sequentially creates trees in a forest, each one learning from the mistakes of the previous one. The first tree builds its nodes, finding the best split at each one using Gini impurity or entropy as in random forest. However, at the end of the tree, the observations are weighted - an increase in weight if misclassified, a decrease if correctly classified. The next tree puts more weight on classifying correctly the observations that were previously misclassified, new weights are calculated at the end of the tree, and the whole process is iterated until the forest finishes.\n\nlibrary(C50)\n\nIt is important to keep in mind that we can overfit our data by setting the amount of trees too high - our model will learn the noise, not the pattern. So, we will iterate through that as well as min_n as before.\n\nabModel &lt;- boost_tree() %&gt;% set_args(trees=tune(), min_n=tune()) %&gt;% \n    set_engine('C5.0') %&gt;% set_mode('classification')\nabWorkflow &lt;- workflow() %&gt;% add_recipe(dataRecipe) %&gt;% add_model(abModel)\n\n\n\nSTEP TWO: TESTING DIFFERENT PARAMETERS\nWe will expand the grid as before.\n\nabGrid &lt;- expand.grid(trees=seq(5,50,5), \n              min_n=c(2,seq(5,25,10)))\nabTuneResults &lt;- abWorkflow %&gt;% tune_grid(resamples=dataCV, grid=abGrid, \n                      metrics=metric_set(accuracy, roc_auc))\nabTuneResults %&gt;% collect_metrics() %&gt;% filter(.metric=='accuracy') %&gt;% \n    arrange(desc(mean)) %&gt;% select(trees, min_n, mean) %&gt;% \n    pivot_longer(cols=c(trees, min_n)) %&gt;% ggplot(aes(x=value,y=mean)) + \n    geom_point() + facet_wrap(~name,scales='free')\n\n\n\n\n\n\n\n\nIt seems like there might be a positive correlation between min_n and accuracy. It’s worth investigating this further.\nNote: This .qmd file is already computationally expensive so I just won’t run this chunk to save processing time. I pasted the output, though.\n\nabGrid2 &lt;- expand.grid(trees=50, min_n=seq(25,100,5))\nabTuneResults2 &lt;- abWorkflow %&gt;% tune_grid(resamples=dataCV, grid=abGrid2, \n                      metrics=metric_set(accuracy, roc_auc))\nabTuneResults2 %&gt;% collect_metrics() %&gt;% filter(.metric=='accuracy') %&gt;% \n    arrange(desc(mean)) %&gt;% select(trees, min_n, mean) %&gt;% \n    pivot_longer(cols=c(trees, min_n)) %&gt;% ggplot(aes(x=value,y=mean)) + \n    geom_point() + facet_wrap(~name,scales='free')\n\n\n\n\nGraph of min_n\n\n\nInterestingly, it seems like the accuracy falls. Oh well, it didn’t hurt to investigate. Let’s finalize our workflow again:\n\nparamFinal &lt;- abTuneResults %&gt;% select_best(metric='accuracy')\nabWorkflow &lt;- abWorkflow %&gt;% finalize_workflow(paramFinal)\n\n\n\nSTEP THREE: FIT THE MODEL\nLet’s fit our model like we did with RF:\n\nabFit &lt;- abWorkflow %&gt;% last_fit(dataSplit)\ntestPerformance &lt;- abFit %&gt;% collect_metrics()\ntestPerformance\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.861 Preprocessor1_Model1\n2 roc_auc     binary         0.914 Preprocessor1_Model1\n3 brier_class binary         0.112 Preprocessor1_Model1\n\n\nOur model has 86.1% accuracy, coincidentally the same as RF from before!\nLet’s make a confusion matrix and plot how our model separates values as before:\n\nabFit %&gt;% collect_predictions() %&gt;% conf_mat(truth=HeartDisease, \n                         estimate=.pred_class)\n\n          Truth\nPrediction   0   1\n         0  80  13\n         1  19 118\n\nabFit %&gt;% collect_predictions() %&gt;% ggplot() + \n    geom_density(aes(x=.pred_1, fill=HeartDisease, alpha=0.5))\n\n\n\n\n\n\n\n\nIn conclusion, we got VERY similar results to our RF model. The accuracy was the same to the tenths place. Onwards to the next model!"
  }
]