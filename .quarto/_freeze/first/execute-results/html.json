{
  "hash": "e36c8050d20fbedae4e7b4772b92bd04",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Stroke Prediction Model\"\nformat: html\neditor: visual\n---\n\n# Stroke Prediction Model\n\n## Introduction\n\nI will be building several models to predict heart disease. I will be using the Heart Failure Prediction Dataset from \\[Kaggle\\] (https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction).\n\nThis web page will be split into several chunks. The first will be for data description and preparation, and the next several will be for each model that I subsequently add.\n\nI will first use a random forest model, then will update with more models periodically.\n\n## Part One: Data Description, Processing, and Preparation\n\nPart one is common to all the models. It's quite brief and simply goes through the pre-processing and data cleaning.\n\n### STEP ONE: LOADING LIBRARIES AND DATA\n\nThis step is pretty self-explanatory. We will make use of six different R libraries for our initial analysis and model. First, `tidymodels` is a collection of packages for modeling and machine learning using `tidyverse` principles. The `workflows` library lets us use workflow container objects that are useful for modeling and analysis projects. The `tune` library will let us iterate through and find the best parameters for our models. The `ranger` library gives us access to a fast and efficient random forest algorithm. Lastly, `ggplot2` lets us make convenient and nice-looking plots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n✔ broom        1.0.9     ✔ recipes      1.3.1\n✔ dials        1.4.1     ✔ rsample      1.3.1\n✔ dplyr        1.1.4     ✔ tibble       3.3.0\n✔ ggplot2      3.5.2     ✔ tidyr        1.3.1\n✔ infer        1.0.9     ✔ tune         1.3.0\n✔ modeldata    1.5.1     ✔ workflows    1.3.0\n✔ parsnip      1.3.2     ✔ workflowsets 1.1.1\n✔ purrr        1.1.0     ✔ yardstick    1.3.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.4     ✔ stringr   1.5.1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(workflows)\nlibrary(tune)\nlibrary(ranger)\nlibrary(ggplot2)\n\norig_data <- RKaggle::get_dataset('fedesoriano/heart-failure-prediction')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 918 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Sex, ChestPainType, RestingECG, ExerciseAngina, ST_Slope\ndbl (7): Age, RestingBP, Cholesterol, FastingBS, MaxHR, Oldpeak, HeartDisease\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\n### STEP TWO: PRELIMINARY EDA, DATA IMPUTATION\n\nLet's take a glance at our data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(orig_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 918\nColumns: 12\n$ Age            <dbl> 40, 49, 37, 48, 54, 39, 45, 54, 37, 48, 37, 58, 39, 49,…\n$ Sex            <chr> \"M\", \"F\", \"M\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\", …\n$ ChestPainType  <chr> \"ATA\", \"NAP\", \"ATA\", \"ASY\", \"NAP\", \"NAP\", \"ATA\", \"ATA\",…\n$ RestingBP      <dbl> 140, 160, 130, 138, 150, 120, 130, 110, 140, 120, 130, …\n$ Cholesterol    <dbl> 289, 180, 283, 214, 195, 339, 237, 208, 207, 284, 211, …\n$ FastingBS      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ RestingECG     <chr> \"Normal\", \"Normal\", \"ST\", \"Normal\", \"Normal\", \"Normal\",…\n$ MaxHR          <dbl> 172, 156, 98, 108, 122, 170, 170, 142, 130, 120, 142, 9…\n$ ExerciseAngina <chr> \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", …\n$ Oldpeak        <dbl> 0.0, 1.0, 0.0, 1.5, 0.0, 0.0, 0.0, 0.0, 1.5, 0.0, 0.0, …\n$ ST_Slope       <chr> \"Up\", \"Flat\", \"Up\", \"Flat\", \"Up\", \"Up\", \"Up\", \"Up\", \"Fl…\n$ HeartDisease   <dbl> 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1…\n```\n\n\n:::\n:::\n\n\nSo, our data has 12 columns. I'll note that our output column (`HeartDisease`) is in 0s and 1s, representing negative and positive for heart disease, respectively. This is what we want to predict.\n\nWe have two different column types, double and character. Let's take a look at the character variables in the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\norig_data %>% select(where(is.character)) %>% apply(2, table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$Sex\n\n  F   M \n193 725 \n\n$ChestPainType\n\nASY ATA NAP  TA \n496 173 203  46 \n\n$RestingECG\n\n   LVH Normal     ST \n   188    552    178 \n\n$ExerciseAngina\n\n  N   Y \n547 371 \n\n$ST_Slope\n\nDown Flat   Up \n  63  460  395 \n```\n\n\n:::\n:::\n\n\nLet's go through these variables. The labeling of each variable is available on the Kaggle Data Card.\n\n`Sex` is skewed towards males; this might be something worth considering when inferring from this dataset.\n\nThere are four different chest pain types (`ChestPainType`) - asymptomatic (ASY), atypical angina (ATA), non-anginal pain (NAP), and typical angina (TA).\n\n`RestingECG` is either LVH (showing probable or definite left ventricular hypertrophy by Estes' criteria), Normal, or ST (having ST-T wave abnormality - T wave inversions and/or ST elevation or depression of \\> 0.05 mV).\n\n`ExerciseAngina` indicates whether or not the patient has exercise-induced angina.\n\nFinally, `ST_Slope` indicates the slope of the peak exercise ST segment (Up: upsloping, Flat: flat, Down: downsloping).\n\nLet's make a new, cleaned data frame where we convert all character features and the output feature to factors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinalData <- orig_data %>% mutate(HeartDisease=as.factor(HeartDisease)) %>% \n\tmutate(across(where(is.character), as.factor))\n```\n:::\n\n\nNow, let's take a look at the non-factor variables.\n\n(Note: figuring out how to loop through and graph numeric variables was a huge pain! Shout-out to ggplot for making visualization easier).\n\nTo make these graphs, we first select the relevant variables, then work the data frame into two columns (this will help us separate by each variable). Then, we can make a plot with ggplot to visualize the distribution of our numeric variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\norig_data %>% select(where(is.double)) %>% pivot_longer(cols=everything()) %>% \n\tggplot(aes(x=value, fill=name)) + \n\tfacet_wrap(~ name, scales = 'free') + geom_histogram(bins=30)\n```\n\n::: {.cell-output-display}\n![](first_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nIt seems like `Age`, `maxHR`, and `RestingBP` are approximately normally distributed. `RestingBP` seemingly has an outlier at 0 (a dead person was counted in the data, probably). `Cholesterol` has a spike at 0, corresponding to missing data. We can probably leave `Oldpeak` as it is. Lastly, `FastingBS` is another binary data type, so we should convert it to factor so it is better represented in our data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinalData <- finalData %>% mutate(FastingBS=as.factor(FastingBS))\n\nfinalData <- finalData %>% mutate(across(Cholesterol, function(i) {\n\t\tif_else(i==0, as.numeric(NA), i)\n\t}))\n```\n:::\n\n\n## Part Two: Random Forest Model\n\n### STEP ONE: CREATING THE MODEL FRAMEWORK\n\nOkay, now we have to split our original dataset into the training and testing subparts. Tidymodels provides a simple framework to do this with `initial_split()`. We also make a cross validation object which will come in handy later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\ndataSplit <- initial_split(finalData, prop = 3/4)\ndataTraining <- training(dataSplit)\ndataTesting <- testing(dataSplit)\ndataCV <- vfold_cv(dataTraining)\ndataCV\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits           id    \n   <list>           <chr> \n 1 <split [619/69]> Fold01\n 2 <split [619/69]> Fold02\n 3 <split [619/69]> Fold03\n 4 <split [619/69]> Fold04\n 5 <split [619/69]> Fold05\n 6 <split [619/69]> Fold06\n 7 <split [619/69]> Fold07\n 8 <split [619/69]> Fold08\n 9 <split [620/68]> Fold09\n10 <split [620/68]> Fold10\n```\n\n\n:::\n:::\n\n\nOkay, great. So we have two new datasets selected at random (`dataTraining` and `dataTesting`) and a cross-validation object. Now, we have to create the framework of the model. We'll do this with the `recipe()` function from tidymodels. Crucially, we specify the model to normalize all numeric columns (subtract the mean of each column and divide by its standard deviation) and impute the missing values in the `Cholesterol` column using the k-nearest neighbors method.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataRecipe <- recipe(HeartDisease ~ ., data=finalData) %>% \n\tstep_normalize(all_numeric()) %>% step_impute_knn(Cholesterol)\n```\n:::\n\n\nSo now we have our \"recipe\" to make the model; it's time to follow through. The `rand_forest()` function initializes a random forest model, then we set parameters with `set_args()`. We set `mtry` (the number of variables evaluated per tree) and `min_n` (the minimum number of observations needed in a node to split the node further - the lower the value, the deeper the tree) to `tune()` - this will allow us to iterate through these values on our cross validation datasets. The `importance` parameter says that we will define the importance of each variable by permutation (I will explain what this is later). Lastly, we tell the model that we want classification (0 or 1) instead of regression (continuous).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrfModel <- rand_forest() %>% set_args(mtry=tune(), min_n=tune()) %>% \n\tset_engine('ranger', importance = 'permutation') %>% \n\tset_mode('classification')\n```\n:::\n\n\nWe make a `workflow` object with our recipe and our model. Let's take a look at it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrfWorkflow <- workflow() %>% add_recipe(dataRecipe) %>% add_model(rfModel)\nrfWorkflow\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_normalize()\n• step_impute_knn()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  min_n = tune()\n\nEngine-Specific Arguments:\n  importance = permutation\n\nComputational engine: ranger \n```\n\n\n:::\n:::\n\n\n### STEP TWO: TESTING DIFFERENT PARAMETERS\n\nThe reason I tune()-d the `mtry` and `min_n` parameters is because these might significantly influence the accuracy and dependability of our model. Now that we have all the required frameworks, we can test parameter values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Good mtry is sqrt(total vars) = sqrt(11) = 3\nrfGrid <- expand.grid(mtry=2:6, min_n=c(1, 3, 5, 10, 15, 20, 30))\nrfTuneResults <- rfWorkflow %>% tune_grid(resamples=dataCV, grid=rfGrid, \n\tmetrics=metric_set(accuracy, roc_auc))\nrfTuneResults %>% collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 70 × 8\n    mtry min_n .metric  .estimator  mean     n std_err .config              \n   <int> <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1     2     1 accuracy binary     0.863    10 0.00698 Preprocessor1_Model01\n 2     2     1 roc_auc  binary     0.928    10 0.00745 Preprocessor1_Model01\n 3     3     1 accuracy binary     0.860    10 0.00702 Preprocessor1_Model02\n 4     3     1 roc_auc  binary     0.928    10 0.00736 Preprocessor1_Model02\n 5     4     1 accuracy binary     0.856    10 0.00857 Preprocessor1_Model03\n 6     4     1 roc_auc  binary     0.924    10 0.00753 Preprocessor1_Model03\n 7     5     1 accuracy binary     0.853    10 0.00908 Preprocessor1_Model04\n 8     5     1 roc_auc  binary     0.922    10 0.00813 Preprocessor1_Model04\n 9     6     1 accuracy binary     0.856    10 0.0105  Preprocessor1_Model05\n10     6     1 roc_auc  binary     0.921    10 0.00768 Preprocessor1_Model05\n# ℹ 60 more rows\n```\n\n\n:::\n:::\n\n\nI'm interested in the effect of `mtry`. Let's take a look across various `min_n`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrfTuneResults %>% collect_metrics() %>% filter(min_n %in% c(1,3,30), .metric=='accuracy')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 15 × 8\n    mtry min_n .metric  .estimator  mean     n std_err .config              \n   <int> <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1     2     1 accuracy binary     0.863    10 0.00698 Preprocessor1_Model01\n 2     3     1 accuracy binary     0.860    10 0.00702 Preprocessor1_Model02\n 3     4     1 accuracy binary     0.856    10 0.00857 Preprocessor1_Model03\n 4     5     1 accuracy binary     0.853    10 0.00908 Preprocessor1_Model04\n 5     6     1 accuracy binary     0.856    10 0.0105  Preprocessor1_Model05\n 6     2     3 accuracy binary     0.863    10 0.00974 Preprocessor1_Model06\n 7     3     3 accuracy binary     0.850    10 0.00759 Preprocessor1_Model07\n 8     4     3 accuracy binary     0.858    10 0.00867 Preprocessor1_Model08\n 9     5     3 accuracy binary     0.862    10 0.00669 Preprocessor1_Model09\n10     6     3 accuracy binary     0.856    10 0.00860 Preprocessor1_Model10\n11     2    30 accuracy binary     0.866    10 0.00869 Preprocessor1_Model31\n12     3    30 accuracy binary     0.859    10 0.00790 Preprocessor1_Model32\n13     4    30 accuracy binary     0.859    10 0.00950 Preprocessor1_Model33\n14     5    30 accuracy binary     0.858    10 0.00865 Preprocessor1_Model34\n15     6    30 accuracy binary     0.856    10 0.00938 Preprocessor1_Model35\n```\n\n\n:::\n:::\n\n\nComparing `mtry`'s for each `min_n`, it seems like there is a small, weak negative correlation between `mtry` and model accuracy. However, `min_n` seems a lot more interesting. Let's keep experimenting. So, let's select the smallest `mtry` (2 and let's try 1 as well) and try a larger range of `min_n`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrfGrid2 <- expand.grid(mtry=1:2, min_n=c(1, 3, 5, seq(5, 50, 5)))\nrfTuneResults2 <- rfWorkflow %>% tune_grid(resamples=dataCV, grid=rfGrid2, \n\tmetrics=metric_set(accuracy, roc_auc))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Duplicate rows in grid of tuning combinations found and removed.\n```\n\n\n:::\n\n```{.r .cell-code}\nrfTuneResults2 %>% collect_metrics() %>% filter(.metric=='accuracy') %>% \n\tarrange(desc(mean))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 24 × 8\n    mtry min_n .metric  .estimator  mean     n std_err .config              \n   <int> <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1     1     3 accuracy binary     0.871    10 0.0110  Preprocessor1_Model03\n 2     1     5 accuracy binary     0.868    10 0.00855 Preprocessor1_Model05\n 3     1    10 accuracy binary     0.868    10 0.00807 Preprocessor1_Model07\n 4     1     1 accuracy binary     0.866    10 0.00841 Preprocessor1_Model01\n 5     2     5 accuracy binary     0.865    10 0.00927 Preprocessor1_Model06\n 6     2    20 accuracy binary     0.865    10 0.00901 Preprocessor1_Model12\n 7     1    15 accuracy binary     0.865    10 0.0100  Preprocessor1_Model09\n 8     1    20 accuracy binary     0.865    10 0.00957 Preprocessor1_Model11\n 9     2    40 accuracy binary     0.863    10 0.00927 Preprocessor1_Model20\n10     2    35 accuracy binary     0.863    10 0.0102  Preprocessor1_Model18\n# ℹ 14 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nrfTuneResults2 %>% collect_metrics() %>% filter(.metric=='roc_auc') %>% \n\tarrange(desc(mean))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 24 × 8\n    mtry min_n .metric .estimator  mean     n std_err .config              \n   <int> <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n 1     2     1 roc_auc binary     0.929    10 0.00771 Preprocessor1_Model02\n 2     2    10 roc_auc binary     0.929    10 0.00741 Preprocessor1_Model08\n 3     2    15 roc_auc binary     0.928    10 0.00749 Preprocessor1_Model10\n 4     2    35 roc_auc binary     0.928    10 0.00798 Preprocessor1_Model18\n 5     1     3 roc_auc binary     0.928    10 0.00805 Preprocessor1_Model03\n 6     2    40 roc_auc binary     0.928    10 0.00799 Preprocessor1_Model20\n 7     2    20 roc_auc binary     0.928    10 0.00755 Preprocessor1_Model12\n 8     2     5 roc_auc binary     0.928    10 0.00794 Preprocessor1_Model06\n 9     2    30 roc_auc binary     0.927    10 0.00775 Preprocessor1_Model16\n10     2     3 roc_auc binary     0.927    10 0.00797 Preprocessor1_Model04\n# ℹ 14 more rows\n```\n\n\n:::\n:::\n\n\nmtry=2, min_n=3 comes in at first place for accuracy, first (tied) for roc_auc, and has lowest std error for accuracy and one of the lowest for roc_auc. It's safe to say that this model is our winner. Turns out we didn't need to investigate again after all!\n\nNow, let's select our winner model and apply it to our workflow.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparamFinal <- rfTuneResults2 %>% select_best(metric='accuracy')\nrfWorkflow <- rfWorkflow %>% finalize_workflow(paramFinal)\n```\n:::\n\n\n### STEP THREE: FIT THE MODEL\n\nSo now we have our workflow:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrfWorkflow\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_normalize()\n• step_impute_knn()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = 1\n  min_n = 3\n\nEngine-Specific Arguments:\n  importance = permutation\n\nComputational engine: ranger \n```\n\n\n:::\n:::\n\n\nNow we can fit this model onto our split data. Tidymodels is nice because it lets you automatically train on the training data and then evaluate on the testing data using the `initial_split()` object. Without further ado, let's fit it and see how it performs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrfFit <- rfWorkflow %>% last_fit(dataSplit)\ntestPerformance <- rfFit %>% collect_metrics()\ntestPerformance\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  <chr>       <chr>          <dbl> <chr>               \n1 accuracy    binary         0.848 Preprocessor1_Model1\n2 roc_auc     binary         0.917 Preprocessor1_Model1\n3 brier_class binary         0.126 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\nAccuracy of 87.8%, not bad!\n\nLet's visualize our results. We'll make a confusion matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrfFit %>% collect_predictions() %>% conf_mat(truth=HeartDisease, \n\testimate=.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction   0   1\n         0  88  15\n         1  20 107\n```\n\n\n:::\n:::\n\n\nOur model on the testing data resulted in 90 true negatives, 112 true positive, 15 false negatives, and 13 false positives.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrfFit %>% collect_predictions() %>% ggplot() + \n\tgeom_density(aes(x=.pred_1, fill=HeartDisease, alpha=0.5))\n```\n\n::: {.cell-output-display}\n![](first_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nWe can see here that we have some pretty neat-looking separation. The plot colors positive outcomes in blue and negative in red. We can see that the model is pretty good at separating the positives from the negatives.\n\n### STEP FOUR: FIT ON A TARGET DATASET\n\nNow we can fit our final model on the dataset and analyze the importance of each variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinalModel <- fit(rfWorkflow, finalData)\nextract_fit_parsnip(finalModel)$fit$variable.importance\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Age            Sex  ChestPainType      RestingBP    Cholesterol \n  0.0068054738   0.0126744834   0.0293602242   0.0021070404   0.0007635453 \n     FastingBS     RestingECG          MaxHR ExerciseAngina        Oldpeak \n  0.0110339096   0.0021859610   0.0132204586   0.0243325816   0.0196821364 \n      ST_Slope \n  0.0610159119 \n```\n\n\n:::\n:::\n\n\nRemember our \"permutation\" argument from before? To find variable `importance`, the model takes each variable one by one, shuffles its values (effectively removing its correlation to the outcome), and evaluates the decrease in the model metric (in this case, accuracy).\n\nTo conclude, our model performed quite well with 87.8% accuracy, an area of 0.937 under the ROC curve, and a Brier score of 0.097. `ST_Slope` seems to be the most important variable, accounting for \\~9% of the model's predictive power.\n",
    "supporting": [
      "first_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}